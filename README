NAME: Forrest Burton
EMAIL: burton.forrest10@gmail.com
ID: 005324612
SLIPDAYS: 2

QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
For 1 and 2-thread list tests, I think most of the cycles are spent on performing the actual list operations because there are only 1 or 2 
threads and therefore very little competition for the CPU. 

Why do you believe these to be the most expensive parts of the code?
I believe these are the most expensive parts of the code because since there are only 1-2 threads. This means the 1-2 threads have to spend 
little to no time waiting for the CPU and locking/unlocking. 

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
I beleive most of the time/cycles for the spin-locks high thread tests is spent spinning as the threads are competing for shared recources. 

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
I beleive most of the time/cycles for the mutex high thread tests is spent waiting for the mutex to be unlocked. This is because trying to  
lock an already locked mutex (conention) is expensive comapred to trying to lock an unlocked mutex. The contention case occurs at a greater
rate with the high-thread mutex tests.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
The lines which are most time consuming are the lines in my thread_tasks function under the spin lock case where the threads are waiting to
obtain the lock before calling the inserting and deleting list functions. 

Why does this operation become so expensive with large numbers of threads?
These operations become expensive with a large number of threads because as the number of threads increases, that leads to increased 
competition for shared recources. As a result, each individual thread spends more time spinning and waiting for the lock to be unlocked.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
The average lock-wait time rises dramatically with the number of threads because more threads are competeting for shared recources
and therefore threads spend more time waiting for the mutex to unlock. 

Why does the completion time per operation rise (less dramatically) with the number of contending threads?
Completion time per operatiion rises less dramatically as the number of contending threads increases because even when there is increased
contention, there are still operations being done on the linked list by the thread that is currently working. Basically the increased 
contention does not really affect the work being done on linked lists at a given time. 

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
It is possible for the wait time per operation to go up faster than the completion time per operation because the list operations done by 
a given thread is not generally affected by the number of threads increasing. On the other hand, when the number of threads increases
this increases the wait time for each thread and therefore makes the wait time per operation to increase higher than the completion time
per operation.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.


The tarball contains:
lab2_list.c:  C program which outputs statistics after initializing an empty list and has the following options: --threads, --iterations, 
              --yield, --sync. The program creates threads then records stastics. The program ouptuts a CSV including: name of test, number of
              threads number of iterations, number of lists, total number of operations performed, total run time (nanoseconds), average 
              time per operation (nanoseconds).
SortedList.c: C module which contains the following methods for implementing a double linked list: insert, delete, lookup, and length
SortedList.h: Header file which describes the API for the double linked list operation
profile.out: profiling report on the program execution. For spin locks, profiling can show us where most of the execution time is spent.
For mutex locks, profiling can only tell us what code is being executed.
lab2_list.csv: Contains the resulting data from tests target 
graphs(.png files): Graphs created utilizing the data from lab2_list.csv and gnuplot(1)
Makefile:     contains options default build, dist, clean, tests, and graphs
lab2b_list.gp: script which generates the .png graphs from parsing lab2_list.csv data 

Recourses:
**Discussion 1B very helpful
Mutex Locks - https://mortoray.com/2019/02/20/how-does-a-mutex-work-what-does-it-cost/
Malloc vs calloc - https://stackoverflow.com/questions/1538420/difference-between-malloc-and-calloc
strlen - https://www.programiz.com/c-programming/library-function/string.h/strlen