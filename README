NAME: Forrest Burton
EMAIL: burton.forrest10@gmail.com
ID: 005324612
SLIPDAYS: 0

QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
For 1 and 2-thread list tests, I think most of the cycles are spent on performing the actual list operations because there are only 1 or 2 
threads and therefore very little competition for the CPU. 

Why do you believe these to be the most expensive parts of the code?
I believe these are the most expensive parts of the code because since there are only 1-2 threads. This means the 1-2 threads have to spend 
little to no time waiting for the CPU and locking/unlocking. 

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
I beleive most of the time/cycles for the spin-locks high thread tests is spent spinning as the threads are competing for shared recources. 

Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
I beleive most of the time/cycles for the mutex high thread tests is spent waiting for the mutex to be unlocked. This is because trying to  
lock an already locked mutex (conention) is expensive comapred to trying to lock an unlocked mutex. The contention case occurs at a greater
rate with the high-thread mutex tests.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.


The tarball contains:
lab2_list.c:  C program which outputs statistics after initializing an empty list and has the following options: --threads, --iterations, 
              --yield, --sync. The program creates threads then records stastics. The program ouptuts a CSV including: name of test, number of
              threads number of iterations, number of lists, total number of operations performed, total run time (nanoseconds), average 
              time per operation (nanoseconds).
SortedList.c: C module which contains the following methods for implementing a double linked list: insert, delete, lookup, and length
SortedList.h: Header file which describes the API for the double linked list operation
profile.out: profiling report on the program execution. For spin locks, profiling can show us where most of the execution time is spent.
For mutex locks, profiling can only tell us what code is being executed.
lab2_list.csv: Contains the resulting data from tests target 
graphs(.png files): Graphs created utilizing the data from lab2_list.csv and gnuplot(1)
Makefile:     contains options default build, dist, clean, tests, and graphs
lab2b_list.gp: script which generates the .png graphs from parsing lab2_list.csv data 

Recourses:
**Discussion 1B very helpful
Mutex Locks - https://mortoray.com/2019/02/20/how-does-a-mutex-work-what-does-it-cost/